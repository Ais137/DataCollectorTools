# 一种组件化的数据处理模块设计

## Index
  * [场景描述](#Question-·-场景描述)
  * [架构设计](#FrameWork-·-架构设计)
  * [特性设计](#Features-·-特性设计)
  * [模块结构](#module-·-模块结构)
  * [核心代码](#Code-·-核心代码)
    * [DataProcessNode](#DataProcessNode(数据处理节点))
    * [DataProcessPipeline](#DataProcessPipeline(数据处理管道))
  * [使用样例](#Usage-·-使用样例)
    * [构建 *DataProcessNode* 数据处理节点](#1.-构建-*DataProcessNode*-数据处理节点)
    * [构建 *DataProcessPipeline* 数据处理管道](#2.-构建-*DataProcessPipeline*-数据处理管道)
    * [数据处理管道的工程化与规范化](#3.-数据处理管道的工程化与规范化)

-------------------------------------------------------
## Question · 场景描述
在数据采集项目中，从采集模块的源数据到具体业务模块需要的数据之间，通常需要通过一个数据处理流程对源数据进行格式转换等一系列操作。
这个数据处理流程对于不同的业务需求有不同的实现，当业务变更时，需要根据新的业务需求修改处理模块，因此，考虑设计一种组件化的数据处理模块来提高扩展性和代码复用率，同时规范化数据处理流程。

-------------------------------------------------------
## FrameWork · 架构设计
模块的主要设计思想来源于 scrapy 框架中 pipeline 数据处理管道的架构和一种类似责任链的设计模式。

模块主要包含两个核心类：***DataProcessNode(数据处理节点)*** 和 ***DataProcessPipeline(数据处理管道)***

***DataProcessNode(数据处理节点)*** 用于将完整的数据流程拆解到单个组件，每个组件封装不同的处理逻辑，比如对字段类型进行转换，缺失值检测，数据过滤等。对于不同的业务可能包含一段相同的处理逻辑片段，因此可以通过这种组件化的架构来提高代码的复用率。同时在具体的实现时，每个节点应该具有单一的处理逻辑。

***DataProcessPipeline(数据处理管道)*** 则用于将 *DataProcessNode(数据处理节点)* 封装成一个完整的数据处理流程，通过组装不同的数据处理节点来覆盖不同的数据处理需求。同时当需求变更时，通过增删组件来实现可扩展性。在进行数据处理时，会让数据流过每个数据处理节点，以实现完整的数据处理流程，同时当某个节点处理异常时，会记录异常的节点和异常信息，用于后续分析。

-------------------------------------------------------
## Features · 特性设计
1. 组件化构建: 通过组件化的方式完成数据处理流程的构建。
2. 处理状态监控: 对数据的处理状态进行监控，记录异常的处理节点和异常信息。
3. 兼容性检测: 通过对函数类型注解的自省，来校验两个数据处理节点之间输入输出数据类型的兼容性
4. 数据测试: 对单条数据进行测试，验证完整的数据处理流程，记录每个节点处理的数据副本，用于后续分析。
5. 文档生成: 自动集成数据处理节点的文档并构建一个总体描述文档

-------------------------------------------------------
## Module · 模块结构
* [data_process_pipeline](./data_process_pipeline.py) : 核心模块
* [data_process_node_comps](./data_process_node_comps.py) : 常用组件
* [sample](./sample.py) : 实例

-------------------------------------------------------
## Code · 核心代码

### DataProcessNode(数据处理节点)
```
@class: DataProcessNode | 数据处理节点
@desc: 
    数据处理节点基类，数据处理组件需要继承并重写 process 方法，
    用于将完整的数据流程拆解到单个组件，每个组件封装不同的单一处理逻辑。
@property: 
    * pid(str): 数据节点ID
@method: 
    * build(staticmethod): 基于函数快速构建 DataProcessNode 类
    * init: 数据处理节点初始化
    * process: 核心处理逻辑
    * exit: 数据处理节点销毁
@standard:
    process 方法的开发规范如下:
    1. 方法参数必须包含类型注解，以便后续框架中进行节点之间的数据兼容性校验。
    2. 文档字符串规范如下:
        @func: 数据处理逻辑简述(必要)
        @desc: 数据处理逻辑的详细描述(可选)
        @input: 数据输入数据样例(必要)
        @output: 数据输出数据样例(必要)
```
### DataProcessPipeline(数据处理管道)
```
@class: DataProcessPipeline | 数据处理管道
@desc: 
    对 **数据处理节点(DataProcessNode)** 进行集成和封装，
    以构建一个完整的数据处理流程，通过组装不同的数据处理节点来
    覆盖不同的数据处理需求。
@property
@method: 
    * init: 初始化数据处理管道
    * process: 数据处理接口
    * exit: 销毁数据处理管道
    * check: 检测数据处理节点规范，数据类型兼容性校验
    * test: 测试完整数据处理流程，并输出完整的数据流
    * doc: 提取和构建数据处理节点文档
```

-------------------------------------------------------
## Usage · 使用样例

## 1. 构建 *DataProcessNode* 数据处理节点
模块提供了两个方案实现 *DataProcessNode* 的构建。

1. 基于继承的方式: 
通过继承 *DataProcessNode* 基类，并重写 *process* 方法来封装处理逻辑。
    ```python
    # 数据字段类型转换
    class DataFieldConverter(DataProcessNode):

        def process(self, data: list) -> dict:
            """
            @func: 对源数据的数据字段类型进行转换
            @desc: 将 count 字段转换成 int 类型
            @input: {"count": "137", ...}
            @output: {"count": 137, ...}
            """
            data["count"] = int(data["count"])
            return data
    ```

2. 基于装饰器的封装:
对于第一种方案，通常适用于处理节点需要维护内部状态的情况(比如去重处理)，对于某些简单的逻辑，继承重写的方案相对比较繁琐，因此提供了一种简便的语法糖，通过 *@DataProcessNode.build* 装饰器，基于 *type* 动态封装和构建 *DataProcessNode* 子类。
被装饰函数将被挂载到 *DataProcessNode.process* 方法上。
    ```python
    # 数据字段类型转换   
    @DataProcessNode.build
    def DataFieldConverter(self, data: list) -> dict:
        """
        @func: 对源数据的数据字段类型进行转换
        @desc: 将 count 字段转换成 int 类型
        @input: {"count": "137", ...}
        @output: {"count": 137, ...}
        """
        data["count"] = int(data["count"])
        return data
    ```
    PS: 这种通过装饰器将普通函数封装成类的方法可能比较激进，对代码的可读性影响不好。

需要注意的是，对于上述两种方案，在实现 *process* 方法(或函数)时，需要添加方法参数的类型注解和文档字符串，主要用于后续的数据类型兼容性校验和文档的自动构建。

## 2. 构建 *DataProcessPipeline* 数据处理管道
通过以下方式构建和实例化 *DataProcessPipeline* 对象
```python
# 构建数据管道
data_pipeline = DataProcessPipeline([
    # 数据去重器
    DataDupeFilter(),
    # 数据字段装换
    DataFieldConverter(),
    # 日期格式化
    DateTimeFormat(),
]).init()
# 处理数据
processed_data = data_pipeline.process(data)
data_pipeline.exit()
```
*DataProcessPipeline* 实现了上下文协议，因此支持 *with* 语法
```python
with DataProcessPipeline([
    # 数据去重器
    DataDupeFilter(),
    # 数据字段装换
    DataFieldConverter(),
    # 日期格式化
    DateTimeFormat(),
]) as data_pipeline:
    processed_data = data_pipeline.process(data)
```

构造器的第一个参数是一个列表，其中按顺序装填 *DataProcessNode* 对象，每条数据会按照这个顺序在每个节点进行处理。

*processed_data* 的格式定义如下: 
```json
{
    //处理成功的数据
    "SUCCES": [
        {
            "data": "处理后的数据",
            "source": "原始数据",
        },
        ...
    ],
    //节点过滤的数据
    "FILTER": [
        {
            "data": "处理后的数据",
            "source": "原始数据",
            "node": "过滤该条数据的处理节点ID",
        },
        ...
    ],
    //处理异常的数据
    "ERROR":  [
        {
            "data": "处理后的数据",
            "source": "原始数据",
            "node": "处理异常的数据处理节点ID",
            "error": "异常信息"
        },
        ...
    ],
}
```
结果集中的 "source" 字段保留 *deepcopy* 深拷贝后的原始数据，用于在处理异常时，进行后续分析。

*init* 的方法调用不是必须的，在设计上其他方法调用时会检测初始化状态并隐式地调用，但是为了可读性，尽量主动调用(该方法支持链式调用)。


## 3. 数据处理管道的工程化与规范化
*DataProcessPipeline* 提供了一下方法来实现数据处理模块的工程化与规范化
  * *check* : 检测数据处理节点(DataProcessNode)规范，数据类型兼容性校验
  * *test* : 测试完整数据处理流程，并输出完整的数据流
  * *doc* : 提取和构建数据处理节点文档

### 1. ***check***
```python
data_pipeline.check()
```
一种“伪静态”的数据类型校验，
通过提取 *DataProcessNode.process* 的函数签名来校验两个节点之间输入输出数据类型的兼容性，
一个处理节点输出的数据类型应该与下一个处理节点输入的数据类型保持一致
  * True:  node(A) -> list:list -> node(B)
  * False: node(A) -> list:dict -> node(B)

对于两个 A，B 数据类型的兼容性定义如下:
  1. A，B 中包含一个任意类型(inspect._empty) 
  2. A，B 的继承链上具有一个共同父类，即存在一个父类 Base，A，B 是 Base 的子类
  3. A 是 B 的子类，或者 B 是 A 的子类。

核心实现:
```python
# 数据类型兼容性测试(inspect._empty作为任意类型标识)
def compatibility(dataTypeA, dataTypeB):
    if (dataTypeA is inspect._empty) or (dataTypeB is inspect._empty):
        return True
    return bool(set(dataTypeA.__mro__[:-1]) & set(dataTypeB.__mro__[:-1]))
```

当 process 方法未包含类型注解时(inspect._empty)，视作任意数据类型。

需要注意的是，该方法只能校验数据的“外层”数据类型，无法检测内部字段的数据类型。

### 2. ***test***
```python
test_data = data_pipeline.test(data, export_filepath="./test.json")
```
对单条数据进行测试，测试完整数据处理流程，并记录每个节点处理的数据副本，用于在开发阶段进行快速测试和修正。

*test_data* 的数据格式如下:
```json
{
    "state": "处理结果状态(SUCCES, FILTER, ERROR)",
    "source": "原始数据",
    "flow": "节点处理的数据副本" -> [
        {"node": "节点ID", "data": "数据副本"}
        ...
    ],
    "data": "处理后的数据"
}
```

### 3. ***doc***
```python
data_pipeline.doc("./doc.md")
```
基于数据处理节点 *process* 方法的文档字符串构建简单的文档，用于快速了解整个数据处理管道
的数据处理流程。

该方法支持 *extract_doc_from_node* 参数用于自定义从每个数据处理节点(DataProcessNode)提取的文档格式。

-------------------------------------------------------
## Meta
```json
{
    "name": "一种组件化的数据处理模块设计",
    "date": "2023-03-03",
    "author": "Ais",
    "tags": ["python", "data", "data_pipeline"],
    "desc": ""
}
```
